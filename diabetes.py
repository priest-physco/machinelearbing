# -*- coding: utf-8 -*-
"""diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SbXLUtEjS8sbjajcADn-o5RlWSCgmkqN
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()
dataset = pd.read_csv('diabetes.csv')
dataset.head(7)

dataset.shape

dataset.isna().sum()

dataset = dataset.dropna(axis=1)

dataset.shape

import seaborn as sns

from sklearn.preprocessing import LabelEncoder
labelencoder_Y = LabelEncoder()
dataset.iloc[:,1] = labelencoder_Y.fit_transform(dataset.iloc[:,1].values)
print(labelencoder_Y.fit_transform(dataset.iloc[:,1].values))

dataset.head(5)

dataset.corr()

X = dataset.iloc[:, 2:31].values
Y = dataset.iloc[:, 1].values

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 0)

from sklearn.preprocessing import  StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

def model(X_train,Y_train):
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state = 0)
  log.fit(X_train,Y_train) 

  from sklearn.neighbors import KNeighborsClassifier
  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
  knn.fit(X_train,Y_train)

  from sklearn.svm import SVC
  svc_lin = SVC(kernel = 'linear', random_state=0)
  svc_lin.fit(X_train, Y_train)

  from sklearn.naive_bayes import GaussianNB
  gauss = GaussianNB()
  gauss.fit(X_train,Y_train)

  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion = 'entropy', random_state=0)
  tree.fit(X_train,Y_train)

  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators = 10, criterion='entropy', random_state=0)
  forest.fit(X_train,Y_train)

  print('[0]Logistic Regression training accuracy:', log.score(X_train,Y_train))
  print('[1]Nearest Neighbor training accuracy:', knn.score(X_train,Y_train))
  #print('[2]Support vector machine (linear classier)training accuracy:', svc_lin(X_train,Y_train))
  #print('[3]support vector machine (linear classier)training accuracy:', svc_rbf(X_train,Y_train))
  print('[4]Gaussian naive bayes training accuracy:', gauss.score(X_train,Y_train))
  print('[5]Decision Tree classifier training accuracy:', tree.score(X_train, Y_train))
  print('[6]Random forest training accuracy:', forest.score(X_train,Y_train))

  return log, knn, svc_lin,  gauss, tree, forest

model = model(X_train,Y_train)

from sklearn.metrics import confusion_matrix
for i in range(len(model)):
  cm = confusion_matrix(Y_test, model[i].predict(X_test))

  TP = cm[0][0]
  TN = cm[1][1]
  FN = cm[1][0]
  FP = cm[0][1]

  print(cm)
  print('Model[{}] Testing Accuracy = "{}"'.format(i, (TP + TN)/(TP + TN + FN + FP)))
  print()